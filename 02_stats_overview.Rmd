
class: inverse, center, middle
# Statistical Landscape of DiNA methods
---

# Timeline 

- I found 40+ methods papers on DiNA methods published in the last 10 years

- The wide variety is due to addressing many subtly different problems

```{r out.width = '100%', echo = FALSE}
knitr::include_graphics("alg_timeline2.png")
```

???
- In 2011, Guo et al. published the work “Joint estimation of multiple graphs” `r Cite(bib, "guo_joint_2011")`, which was followed by several new methods to estimate and test differences in biological networks `r Cite(bib, "cai_constrained_2011")` `r Cite(bib, "mohan_structured_2012")` `r Cite(bib, "jacob_more_2012")`

- Danaher et al. introduced group graphical lasso (GGL) and fused graphical lasso (FGL) in 2014 `r Cite(bib, "danaher_joint_2014")`, which was closely followed by other groups proposing direct estimation of the differences between graphs `r Cite(bib, "zhao_direct_2014")`, efficient structural estimation of multiple graphs `r Cite(bib, "zhu_structural_2014")`, and node-based learning of differential graphs `r Cite(bib, "mohan_node-based_2014")`

- Peterson, Stingo, and Vanucci introduced Bayesian inference for multiple graphical models in 2015 `r Cite(bib, "peterson_bayesian_2015")`. Subsequent Bayesian methods developed priors that account for the high dimensionality and sparsity common in biological data  `r Cite(bib, "tan_bayesian_2017")` `r Cite(bib, "richard_li_bayesian_2019")` `r Cite(bib, "sekula_single-cell_2021")`



---
# Why so many methods?

To address various data and modeling situations!
--
.pull-left[
What's your data like?
```{r out.width = '70%', echo = FALSE}
knitr::include_graphics("proscons.png")
```
]

.pull-right[
Other model considerations...
```{r out.width = '70%',echo = FALSE}
knitr::include_graphics("specifications.png")
```
]

---
# DiNA Methods Summary

.small[
- Gaussian: 
  - Graphical Lasso: JGL (Guo 2011)
      - Additional penalties for encouraging similar sparsity across groups: FGL & GGL (Danaher 2014)
            - Incorporating structural information: JSEM (Ma 2016)
                - Extension which doesn't require post-processing: jewel (Angelini 2021)
            - Doesn't require sparse inputs: DTrace (Yuan 2017)
                - Extension for multi-modal data: pDNA (Zhang 2017)
      - Node-based learning framework: PNJGL (Mohan 2014)
      - Unbalanced groups: JAGL (Shan 2018)
      - Hierarchical structure: JWLGL (Shan 2020)
  - Graphical Ridge: TFRE (Bilgrau 2020)
  - Adjust for global conditional dependencies to identify "driver" group-specific components: Dingo (Ha 2015)
  - Group-wise heterogeneous structure: LASICH (Saegusa 2016)
  - Direct estimation of difference: Zhao 2014
  - Uses latent nodes: Na 2019
  - Simultaneous clustering & GM estimation: SCAN (Hao 2017), Price 2021
]

---
# DiNA Methods Summay (cont'd)
.small[
- Non-Gaussian: SPIEC-EASY (Kurtz 2015), pDNA (Zhang 2017)
- Semi-parametric: Xu 2016
- Comparison across 3+ groups: BioNetStat (Jardim 2019)
- Group-wise structure: JMMLE (Majumdar 2022)
- High dimensional: JointGES (Wang 2020), FUDGE (Zhao 2022)
- Bayesian: Peterson 2015, Mitra 2016, Tan 2017, Li 2019, Sekula 2022
]

???
    - Is data Gaussian vs non Gaussian?
    - High dimensional vs low dimensional?
    - Do you want to use Frequentist vs Bayesian framework?
    - Is there local common structure or group-wise heterogeneous structure?
    - Do you need to estimate on 3+ groups? 
    - Are precision matrices sparse or not?

- You can further narrow down specific methods based on if you care about:

    - Hierarchical structure
    - Additional penalties or weighted penalties for unbalanced groups
    - Possibility to cluster and estimate GMs simultaneously
    - Computational efficiency
    - Incorporating known network structure or latent variables

---
# Graphical Lasso (gLasso)

* Because majority of available methods are some variation on gLasso, I'm going to go into the details of the optimization problem and penalty terms here. 

* Convex optimization problem for graphical lasso, where $\lambda$ is a tuning parameter and $||\Theta||_1$ is the sum of absolute values of the elements of $\Theta$. The solution gives an estimate for $\Sigma^{-1}$, the precision matrix:

$$maximize_\Theta\{logdet\Theta - tr(S\Theta) - \lambda||\Theta||_1\}$$
* *Graphical lasso* can be used even when $p >> n$, and when $\lambda$ is large then it forces the estimated precision matrix to be sparse (so few edges!).

* Joint graphical lasso builds upon this by estimating *multiple, related GGMs* from data with observations belonging to distinct classes (for example, cancer vs normal tissue). 

* The idea is to leverage information across the classes while still letting there be class-specific edges. Sparsity and similarity between graphs modified by penalty functions. 

---
# Notation

* $K$ number of classes 2+.  Index classes using $k$ = 1, ... $K$. 
* $\Sigma^{-1}_k$: True precision matrix for the kth class 
* $Y^{(k)}$: $n_k$ x $p$ matrix consisting of $n_k$ observations from the $k$th class on a set of $p$ features which are common to all $K$ data sets  
* $S^{(k)}$: Empirical covariance matrix for $Y^{(k)}$  
* $\Theta^{(k)}$: argument to convex optimization problem used for estimating $\Sigma^{-1}_k$  
* Index matrix arguments by using $i$ = 1, ..., $p$ and $j$ = 1, ..., $p$  
* $\lambda_1$ and $\lambda_2$: non-negative tuning parameters used in penalty function

---

# Major assumptions

* We assume the observations **within** each class are iid. 
* Also assume $\mu_k$, the mean for each class, is 0. i.e:   

$$Y^{(k)}_1, ..., Y^{(k)}_{nk} \sim N(0, \Sigma_k)$$

---
# Optimization problem for Joint Graphical Lasso

* Our goal is to estimate $\Sigma^{-1}_1$, ..., $\Sigma^{-1}_K$ by using penalized log-likelihood approach. 

* Again, we want each class to have it's own precision matrix, but to be able to use information across the classes to make them. 

* Seek $\hat{\Theta}$ by solving:

$$maximize_{\{\Theta\}}\left(\sum^{K}_{k=1}n_k[log\{det(\Theta^{(k)})\}-tr(S^{(k)}\Theta^{(k)}) - P(\{\Theta\})\right)$$

* A **major innovation of the Danaher 2014 paper**, is the generalization of the optimization problem to multiple classes, in addition to using the penalty function $P(\{\Theta\})$, for which the authors provide two different versions. 

---

# Penalty functions

* The general form for the penalty function is:

$$P(\{\Theta\}) = \lambda_1\sum^K_{k=1}\sum_{i \neq j}|\theta^{(k)}_{ij}| + \widetilde{P}\{\Theta\}$$

* Notice that the $P(\{\Theta\})$ is **not class specific**. It takes information from all the classes!

* The form of this penalty function will encourage the solutions to share certain characteristics such as locations of sparsity or value. 

* Depending on the form we choose and the value of the tuning parameters, we could essentially force joint graphical lasso to just perform unrelated graphical lasso on each of the $K$ classes (i.e. if $\widetilde{P}\{\Theta\}$ is zero.)

* Let's look at the possible forms for $\widetilde{P}\{\Theta\}$!

---

# Fused Graphical Lasso

* Fused Graphical Lasso (FGL) uses the following penalty function:

$$P(\{\Theta\}) = \lambda_1\sum^K_{k=1}\sum_{i \neq j}|\theta^{(k)}_{ij}| + \lambda_2\sum_{k<k'}\sum_{i,j}|\theta^{(k)}_{ij}-\theta^{(k')}_{ij}|$$

* When $\lambda_1$ is **large**, FGL makes sparse estimates of $\hat{\Theta}^{(1)}, ... , \hat{\Theta}^{(K)}$ 
* When $\lambda_2$ is **large**, many elements of $\hat{\Theta}^{(1)}, ... , \hat{\Theta}^{(K)}$ will be the same across classes  
* So, FGL "borrows information aggressively across classes, encouraging similar network structure and similar edge values"

---

# Group Graphical Lasso

* Group Graphical Lasso (GGL) uses the following penalty function:

$$P(\{\Theta\}) = \lambda_1\sum^K_{k=1}\sum_{i \neq j}|\theta^{(k)}_{ij}| + \lambda_2\sum_{i \neq j}\left(\sum_{i,j}{\theta^{(k)}_{ij}}^2\right)^{1/2}$$

* Lasso penalty applied to elements of the precision matrices  
* Group lasso penalty is applied to the (i, j) element across all K precision matrices
* When $\lambda_1$ is **large**, GGL makes sparse estimates of $\hat{\Theta}^{(1)}, ... , \hat{\Theta}^{(K)}$ 
* So, GGL just encourages a shared pattern of *sparsity*, not shared *edge values* (unlike FGL which encourage sharing across both)

---
# Other penalty functions

- Here is a selection of penalty functions for comparison. For full treatment see Tsai or Shojaie review papers. 

```{r out.width = '70%', echo = FALSE}
knitr::include_graphics("penalties.png")
```

---
# Bayesian Inference of Multiple GGMs

* Peterson, Stingo, and Vannucci (2015) offer an alternative Bayesian approach.

* They use Markov random field (MRF) prior which encourages common structures across groups, but does not assume that all subgroups are related

* Place a spike-and-slab prior on parameters that measure network relatedness in an effort to learn which groups have a shared graph structure

* Posterior probabilities of inclusion for those parameters summarize the network similarity.

---
# Intuition behind spike-and-slab prior

```{r out.width = '90%',echo = FALSE}
knitr::include_graphics("priors.png")
```

---
# Intuition behind spike-and-slab prior

```{r out.width = '90%',echo = FALSE}
knitr::include_graphics("prior_spike.png")
```

---
# Notation

* $K$ number of classes 2+.  Index classes using $k$ = 1, ... $K$. 
* $\Sigma^{-1}_k$: True precision matrix for the kth class 
* $Y^{(k)}$: $n_k$ x $p$ matrix consisting of $n_k$ observations from the $k$th class on a set of $p$ features which are common to all $K$ data sets. $n_k$ need not be identical size.  
* $S^{(k)}$: Empirical covariance matrix for $Y^{(k)}$  
* $\Omega^{(k)}$: symmetric positive definite matrix constrained by a graph $G_k$
* $\mathbf{g}_{ij}$: Binary vector representing inclusion of edge (i, j) in graphs 1, ..., K.
* Index matrix arguments by using $i$ = 1, ..., $p$ and $j$ = 1, ..., $p$  

---

# Markov Random Field Prior

Probability of the binary vector of edge inclusion indicators $\mathbf{g}_{ij}$ given by:

$$p(\mathbf{g}_{ij}|v_{ij}, \Theta) = C(v_{ij}, \Theta)^{-1}exp(v_{ij}\mathbf{1}^Tg_{ij}+\mathbf{g}_{ij}^T\Theta\mathbf{g}_{ij})$$

* $\mathbf{1}$: unit vector of dimension K
* $v_{ij}$ is a parameter specific to each set of edges $\mathbf{g}_{ij}$
* $\Theta$ is a KxK symmetric matrix representing pairwise relatedness of the graphs for each sample group. 
  * Diagonals are zero, off-diagonals which are non-zero represent connections between related networks.
* $C(v_{ij}, \Theta)$: Normalizing constant
---

# MRF continued

So, prior probability that edge (i, j) is absent from all K graphs is: 

$$p(\mathbf{g}_{ij} = 0|v_{ij}, \Theta) = 1/C(v_{ij}, \Theta)$$
* Joint prior on graphs is product of densities for each edge:

$$p(G_1, ...G_K|v, \Theta) = \prod_{i<j}p(\mathbf{g}_{ij} = 0|v_{ij}, \Theta)$$
Conditional probability of the inclusion of edge in $G_k$, given inclusion of edge in remaining graphs, is:

$$p(g_{k,ij}|{g_{m,ij}}_{m\neq k}, v_{ij}, \Theta) = \frac{exp(g_{k,ij}(v_{ij}+2\sum_{m\neq k}\theta_{km}g_{m,ij}))}{1+exp(v_{ij}+2\sum_{m\neq k}\theta_{km}g_{m,ij})}$$

---
# Selection prior on network similarity 

Impose priors on $v$ and $\Theta$ to reduce false selection of edges.

* Use spike-and-slab prior on the off-diagonal entries of $\Theta$: $\theta_{km}$
  * "Slab" portion defined on positive domain
  * Appropriate choice is $Gamma(x|\alpha, \beta)$ with $\alpha > 1$
  * So mixture prior on $\theta_{km}$ is $p(\theta_{km}|\gamma_{km})$
  * Joint prior on off-diagonal entries is product of marginal densities:
    * $p(\Theta|\gamma) = \prod_{k<m}p(\theta_{km}|\gamma_{km})$
  * Place Bernoulli prior on latent indicators

---
# Edge-specific informative prior on v

* Given a known reference network $G_0$, define a prior that encourages higher selection probabilities for edges included in $G_0$.

* Impose a prior on probability of inclusion in edge in $G_k$ which reflects belief that it is similar to the reference network. Use Beta(a,b) prior. 

* In cases where no prior knowledge on graph structure, use a prior that favors lower values such as Beta(1, 4) to encourage overall sparsity.

---

# Joint Posterior

* Joint posterior given $\Psi$ is set of all parameters and $X$ is observed data for all groups:


```{r out.width = '90%',echo = FALSE}
knitr::include_graphics("Spike_And_Slab.png")
```

???
$p(\Psi|X) \propto \prod_{k=1}^K[p(X_k|\mu_k,\Omega_k)p(\mu_k|\Omega_k)p(\Omega_k|G_k)]\prod_{i<j}[p(g_{ij}|v_{ij}, \Theta)p(v_{ij})]p(\Theta|\gamma)p(\gamma)$

---
# MCMC Sampling

* Since posterior distribution is analytically intractable, construct Markov chain Monte Carlo (MCMC) sampler to obtain a posterior sample of the parameters of interest.
  * At iteration t:
    * Update graph $G_k^{(t)}$ and precision matrix $\Omega_k^{(t)}$ for each group k = 1, ..., K
    * Update parameters for network relatedness $\theta_{km}^t$ and $\gamma_{km}^t$ for 
$1 \leq k < m \leq K$
    * Update edge-specific parameters $v_{ij}^{(t)}$ for $1 \leq i < j \leq p$
    
---
# Other Bayesian methods for inferring multiple GGMs

* Method for non-normal and mixed discrete continuous 'omic data (Bhadra 2018)

* Bayesian counterpart of JGL with simultaneous shrinkage and model selection (Li 2019)

* Multi-layered genomic networks - good for when you have multiple data types/hierarchical structure (Ha 2020) 

* Hierarchical Bayesian factor model for count data (good for single-cell differential network analysis) (Sekula 2021)

---
# Takeaway from methods landscape

* There are many available methods to jointly estimate multiple graphical models

* Choice of method depends on your data type, goal of analysis, and ease of implementation (more on that in software section)

---
class: inverse, center, middle
```{r out.width = '80%',echo = FALSE}
knitr::include_graphics("quantifydif.png")
```

---
# Some Node Importance Measures

```{r out.width = '80%',echo = FALSE}
knitr::include_graphics("node_importance.png")
```

---
class: inverse, center, middle

# And finally... test the difference

---

# Methods & Software for testing

  - Lichtblau 2017 Compares 10 methods for quantifying node-specific differences between groups
  - Identify pairs of nodes with difference (Ha DINGO 2015, McKenzie DGCA 2016)
  - Identify subsets (3+) nodes that whose connections are different between groups (Jardim BioNetStat 2019, Arbet PND 2021)

- Various p-value options, e.g. permutation  

- Adjust for multiple testing!! Bonferoni for conservative estimate, FDR for less stringent. 

- For Bayesian methods: posterior mean and $100(1-\alpha)\%$ Credible Interval for each gene-gene pair correlation difference are obtained from the posterior. (Sekula 2022)



